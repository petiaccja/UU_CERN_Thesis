\documentclass[12pt]{article}

\usepackage{float}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{subcaption}
\usepackage{graphicx}
\graphicspath{ {./images/} }

\usepackage{listingsutf8}
\usepackage[utf8]{inputenc}

\usepackage{xcolor}
 
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\fontsize{7}{9}\selectfont\ttfamily,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}

\usepackage[a4paper, total={16cm, 23.7cm}]{geometry}
\DeclareMathSizes{12}{13}{10}{8}
\setlength\parindent{0.7cm}

\usepackage[colorlinks=true,urlcolor=blue]{hyperref}
\usepackage{color}

\newcommand*\diff{\mathop{}\!\mathrm{d}}
\newcommand*\Diff[1]{\mathop{}\!\mathrm{d^#1}}
\newcommand\tab[1][.7cm]{\hspace*{#1}}
\renewcommand{\refname}{}

\captionsetup{width=0.8\textwidth}

\newcommand{\code}[1]{\texttt{#1}}

\fontfamily{phv}

% !TeX spellcheck = en_US


\begin{document}	

%----------------------------------------------------------------------------
% TITLE
%----------------------------------------------------------------------------
\begin{center}
	\Huge Performance optimization of the online data processing software of CERN's LHCb experiment\\
	\Large Thesis report\\
	\vspace{1pc}
	\huge PÃ©ter Kardos \\
	\large 2018-2019
\end{center}


%----------------------------------------------------------------------------
% === ABSTRACT ===
%----------------------------------------------------------------------------
\section{Abstract}

\color{red}
write at the end \\
100-200 words \\
- what's the problem \\
- how was it solved \\
- what are the results \\
- conclusion: what it means for the future \\
must be understandable without extra info	
\color{black}
\vspace{1.5pc}
	
\textcolor{cyan}{Don't read this, it's just a placeholder.}
So this abstract should be about 100-200 words so I'm just writing some natural text to act as a placeholder. By looping this text a few times, I can probably make a 150 word section. So this abstract should be about 100-200 words so I'm just writing some natural text to act as a placeholder. By looping this text a few times, I can probably make a 150 word section. So this abstract should be about 100-200 words so I'm just writing some natural text to act as a placeholder. By looping this text a few times, I can probably make a 150 word section. So this abstract should be about 100-200 words so I'm just writing some natural text to act as a placeholder. By looping this text a few times, I can probably make a 150 word section. So this abstract should be about 100-200 words so I'm just writing some natural text to act as a placeholder. By looping this text a few times, I can probably make a 150 word section.


%----------------------------------------------------------------------------
% === Introduction ===
%----------------------------------------------------------------------------
\newpage
\section{Introduction}\label{sec_intro}

\color{red}
describe the problem in detail \\
specific to my thesis: \\
environment: \\
- CERN's goals/activity \\
- CERN's hardware infrastructure (accelerators, experiments) \\
- LHCb's hardware infrastructure \\
- LHCb's software reconstruction system \\
problem: \\
- event rate from detector \\
- slow trigger $\rightarrow$ loss of physics (ACTUAL PROBLEM) \\
- by optimizing individual algorithms (in this thesis) \\
\color{black}
\vspace{1.5pc}

\subsection{About CERN}\label{sec_about_cern}

CERN (European Organization for Nuclear Research) is an international high energy experimental physics research organization situated near Geneva, on the Franco-Swiss border. CERN is host to the world's largest particle accelerator and numerous experiments which aim to provide a better understanding of the universe. The goals of the experiments, among others, are to verify the standard model of particles. \textcolor{cyan}{TODO: list more concrete goals}.
\cite{cern_about}

\subsection{What are particle accelerators}\label{sec_part_accel_intro}

\subsubsection{Idea and purpose of accelerators}\label{sec_part_accel_idea}
Particle accelerators, as the name suggests, accelerate charged particles to extremely high velocities. Generally, the accelerated particles are elementary particles, such as protons or electrons, or ions. CERN mainly operates with protons and lead ions, though we can see other particles as well. The velocity of the particles often approaches that of the speed of light in vacuum. CERN's first particle accelerator, the Synchrocyclotron, reached about 80\% of the speed of light, whereas CERN's largest accelerator's, the LHC, accelerates its protons to 99.9999991\% \cite{lhc_facts_and_figures} of the speed of light.

The high-speed particles are made to collide with a stationary target or each other (particles of opposing directions having a frontal collision), which results in a shower of new-born particles flying away from the collision point. In high-energy collisions, exotic particles that are normally not seen are born, and their properties can be examined to advance the scientific field of particle physics.

\subsubsection{Theory of operation}\label{sec_part_accel_theory}

\begin{figure}[H]
	\begin{center}
		\includegraphics[width=0.5\textwidth]{particle_accelerator_schematic}
	\end{center}
	\caption{Simplified schematic of a circular particle accelerator with the crucial functional parts labeled.}
	\label{fig_part_accel_schematic}
\end{figure}

Particle accelerators can be circular or linear. Figure \ref{fig_part_accel_schematic} depicts a circular accelerator, which consists of the following parts:
\begin{itemize}
	\item Evacuated tube (beam pipe): a closed, circular tube in which the particles can travel. To avoid the particles colliding with air particles, it is strictly under vacuum.
	\item Particle source: injects the particles into the beam pipe. For proton accelerators, a bottle of hydrogen serves as the source. The hydrogen atoms are ionized, and then linearly accelerated by an electric field before entering the circular beam pipe.
	\item Accelerating cavity: uses oscillating electromagnetic fields which are timed correctly to provide a push to the charged particles via electric force, accelerating them.
	\item Deflecting magnet: using the Lorentz-force, a strong magnetic field steers the particles inwards to the center of the circle, keeping it on the circular trajectory.
	\item Extraction pipe: once the particles are fast enough, they are extracted from the circular beam pipe to collide with a stationary target.
\end{itemize}

The particle is at first injected to the beam pipe at a low energy. Thanks to the deflecting magnets, it keeps revolving in the beam pipe for thousand of revolutions. Each turn, particles get boosted by the accelerating cavities as they pass by, increasing their energy. As the energy increases, deflecting magnets have to work more with stronger fields to keep them on track. When particles reach their maximum energy, which is determined by the construction and size of the accelerator, they are extracted from the beam pipe to collide with a stationary target.

In fact, it is not individual particles that revolve around the accelerator, rather, it's a swarm of thousands or millions of particles distributed throughout the entire circle, forming a so called beam. The distribution, however, is not continuous. The particles group into \textit{bunches} which are equally spaced throughout the circle. The LHC contains 2808 bunches, each consisting of $1.15\cdot10^{11}$ protons.

In addition to deflecting magnets, accelerators also employ \textit{focusing magnets}. Their purpose is to squeeze the bunches in the directions perpendicular to the particles' velocity, resulting in the beam having a smaller cross-section. Without focusing magnets, the bunches would slowly disintegrate and the particles would hit the wall of the beam pipe.

Linear accelerators contain the same functional elements as circular ones, except for the fact that they don't need deflecting magnets since the particles travel in a straight path.

\subsubsection{Particle colliders}\label{sec_part_collider}

As opposed to particle accelerators, colliders operate with two beams at the same time. A circular collider, such as the LHC, has two beam pipes right next to each other in the circular tunnel. The two beams circle in the opposing directions. At some specific points, the two beam pipes are made to cross, and as the particle beams intersect, the bunches collide with each other as opposed to colliding with a fixed target.


\subsection{The accelerator complex \cite{cern_accel_complex}}\label{sec_accel_complex}

\begin{figure}[H]
	\includegraphics[width=\textwidth]{accelerator_complex}
	\caption{Schematic view of CERN's particle accelerators and experiments. LHC is shown on top by the largest circle. The four main experiments, CMS, ALICE, ATLAS and LHCb are marked with yellow dots along the LHC's circle.}
	\label{fig_accel_complex}
\end{figure}

As seen on \ref{fig_accel_complex}, CERN has a quite complex system of particle accelerators. We can see several circular and linear accelerators, and also some deccelerators, all of which are running at the same time in a synchronized fashion so that they can interact with each-other.

The largest circle, in dark blue, is LHC, which is the world's largest and most powerful particle collider to date. The way protons reach their final energy in the LHC is a complex, multi-stage process. The protons are sourced from a bottle of hydrogen, and injected into LEIR via LINAC 3. When the energy of the protons reaches the maximum operating point of LEIR, they are transferred into PS for further boosting. When they reach PS's operating maximum, they are transferred into SPS, and finally into the LHC.

Inside the LHC, we can find two beams going in the opposite directions. The energy of each particle reaches about 7 TeVs at maximum. The two beams cross each other and particles collide at 4 points, marked CMS, ALICE, LHCb and ATLAS. At these points, complex particle detectors are installed to analyze the collisions. Detectors track the path of the particles, and make measurements on their properties such as momentum, charge and mass. From the collection of properties, particles can be identified. Measurements provide valuable data to physicists who are trying to verify and extend the standard model of particles. In most cases, the raw data provided by the detectors is processed by software.

\subsection{LHCb experiment's detector}\label{sec_lhcb_detector}
	
\subsubsection{Construction of the detector}\label{sec_lhcb_det_constr}

\begin{figure}[H]
	\includegraphics[width=\textwidth]{lhcb_geometry_upgrade}
	\caption{Side view of the LHCb detector.}
	\label{fig_lhcb_geometry}
\end{figure}

Figure \ref{fig_lhcb_geometry} shows the LHCb detector from the side, which means that the two beams of LHC are going in horizontal directions on the drawing through the middle of the detector. The beam pipe coincides with the horizontal axis of symmetry of the detector.

As seen on the labels, the detector consists of multiple layers of sub-detectors. Each layer has a hole in the center to let the beam pipes through. The two particle beams cross each other inside the Vertex Locator (VELO, at the right in yellow). As opposed to most other detectors, this one only analyzes the products of the collision in a narrow cone away from the VELO. The parts from RICH to M5 could be mirrored around the VELO to have two cones that touch each other by the tip, however it is not done for financial reasons.

The goal of the LHCb detector is the same as for all other detectors: reconstruct the paths and types of the particles. Even though full reconstruction uses all sub-detectors, we are only interested in partial reconstruction that can be done in real-time. This only involves the VELO, the UT (in orange, left of VELO) and the FT (SciFi Tracker, in the middle in orange).

\subsubsection{Coordinate system}\label{sec_coordinate_system}

When doing calculatings, the detector must be placed in a 3 dimensional euclidean space. The coordinate axis are chosen so that Z down the beam pipe from the VELO towards the SciFi tracker, X points to the right when lookind down the Z axis, and Y points upwards.

\textcolor{red}{TODO: add a picture}


\subsubsection{Operating principles}\label{sec_lhcb_det_theory}

\begin{figure}[H]
	\includegraphics[width=\textwidth]{lbevent_collision_example}
	\caption{Particles created in a real collision and their interaction with the detector.}
	\label{fig_lhcb_lbevent_collision}
\end{figure}


Figure \ref{fig_lhcb_lbevent_collision} shows one particle collision event's results. The particles that were born in the collision are shown by the orange lines. The three aforementioned detectors, the VELO, UT and FT, can be identified by the origin of the orange particles, the pink cloud of lines and the bright blue cloud lines, respectively. The green and yellow cubic illustrations on the far-end of the detector belong to other sub-detectors, and are out of the scope of this paper.

The VELO is a small detector, measures less than a meter in length. It is a silicon pixel detector, which looks much like a modern CCD camera: a rectangular array of pixels which detect light (or in this case, particles) that hit it. The difference is that particle detectors don't absorb the particle, rather, it passes through largely undisturbed. Additionally, the VELO consists of 26 such CCD-like rectangles, instead of just one.

\begin{figure}[H]
	\begin{center}
		\includegraphics[width=0.5\textwidth]{detector_velo_real}
	\end{center}
	\caption{The 26 layers of the VELO. The pixels reside on the silver area, the PCB around contains the reading electronics, and the wedge at the top-center of the silver area corresponds to the beam pipe.}
	\label{fig_detector_velo_real}
\end{figure}

As opposed to the VELO, the UT is a silicon strip detector. Silicon strips detectors consist of not pixels, but from long fibers which act much like a pixel in the sense they also signal if a particle passes through them. A fiber of the UT is generally 10 centimeters long, and has a width of only 192 micrometers. This means that while on one axis, the particles position can be told with great accuracy, on the other axis the uncertainty if 10 centimeters. The entire detector measures about 1.7 meters in width and 1.4 meter in height.

Similarly to the UT, the FT is also a silicon strip detector, but the length of it's fibers is 2.5 meters. This means the 5 meter tall detector needs only two fibers to cover the full height.

\begin{figure}[H]
	\begin{center}
		\includegraphics[width=0.5\textwidth]{detector_ut_ft_drawing}
	\end{center}
	\caption{The UT in purple and the FT in blue, with a person next to them to illustrate the scale. The silicon strips are aligned vertically, leading to a good horizontal resolution but a poor vertical resolution.}
	\label{fig_detector_ut_ft_drawing}
\end{figure}

In between the UT and the FT, a strong magnet is placed which bends the particles on the horizontal axis.

When a collision occurs, the particles follow a straight path and they are recorded passing through the pixels of the VELO. Initial particle trajectories can be reconstructed by finding hits in the VELO that align to form a straight line. These paths are then linearly extrapolated through the UT, and some of the silicon strips that were lit up by the particles are assigned to the initial trajectories acquired from the VELO. Inside the UT, particles paths are mostly straight, but they already experience a slight bending due to the magnet after the UT. From the amount of bending, the particle's momentum can be estimated. As the charged particles pass through the magnet, their trajectory bends, however, the amount of bending is a function of the particle's momentum. With a good momentum estimate, a guess can be given as to where the particle would hit the FT. In the suspected region, the silicon strips of the FT are searched and if the corresponding fibers are found, they are assigned to the particle.

At the end of the process, the all the pixels or fibers that were touched by a particle are known, which makes it possible to know the exact path of the particle. The amount of bending from the UT to the FT allows the calculation of the momentum, which, when paired with information from other detectors, such as energy and velocity, makes it possible to identify a particle. (Identification means knowing the name of the particle, such as electron or muon.)

\subsection{Events and triggering}\label{sec_events_trigger}

\subsubsection{Collision events}\label{sec_event_what}

As explained earlier, the LHC has 2808 bunches of protons circulating in both directions in the two beam pipes. We refer to the collision of the of these bunches as an \textit{event}. Events are completely independent, that is, a bunch-bunch collision only happens after the previous collision's products have been analyzed and flushed from the detector. At nearly the speed of light, a particle takes 89 microseconds to do a revolution in the circular detector of 26 659 m circumference. The 2808 bunches are however spaced at a distance of 25 nanoseconds from each-other, giving way to at most 40 million collisions every second. Calculating with 2808 bunches and 89 microseconds, the bunch spacing should be 31.7 nanoseconds. There are, however, some longer gaps in the line of 2808 bunches, so two adjacent bunches are still 25 nanoseconds away. This is why, in practice, there are only roughly 30 million collisions a second, but the processing of that has to happen as fast as 40 million a second.

\subsubsection{Real-time reconstruction and triggering}\label{sec_trigger_what}

Most of the 30 million events that occur every second are absolutely uninteresting, with no exotic particles of interest being created. On the other hand, each event amounts to a significant amount of data which is a challenge to store. To reduce the amount of data to be stored, each event is processed in real-time to determine weather it is interesting or not. Uninteresting events are simply dropped, the interesting ones go to long-term storage for further analysis. The act of deciding if an event has to be stored is called \textit{triggering}. The real-time processing of events is a simplified method that relies on the VELO, UT and FT detectors as described previously.


\subsection{The 2019/20 upgrade of LHCb}\label{sec_lhcb_upgrade}

The LHC shuts down for maintenance for two years from the end of 2018. The LHCb detector also undergoes maintenance and upgrade during that time, where the VELO is upgraded and the old TT is entirely replaced by the UT.

The pre-upgrade detector does the real-time event processing in two stages. The first stage employs FPGAs to do a preselection, which cuts down the 30 million events per second to roughly 1 million per second. The second stage uses a large farm of CPUs to do finer reconstruction and final trigger decision. During the upgrade, however, the FPGAs will be retired, and the CPU farm has to take the whole load of 30 million events per second. This puts a stress on the software stack that it was not written to handle, and needs a significant overhaul.

\begin{figure}[H]
	\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=.4\linewidth]{LHCb_Trigger_RunII_May2015}
		\caption{Trigger data flow scheme\\ before the upgrade}
		\label{fig:sub1}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=.4\linewidth]{LHCb_Trigger_RunIII_May2015}
		\caption{Trigger data flow\\ scheme post-upgrade}
		\label{fig:sub2}
	\end{subfigure}
	\caption{Comparison of the two triggering solutions}
	\label{fig_trigger_compare}
\end{figure}


\subsection{Overview of the real-time processing software}\label{sec_reco_sw_overview}
	
\begin{figure}[H]
	\begin{center}
		\includegraphics[width=0.6\textwidth]{algorithms_brunel}
	\end{center}
	\caption{Simplified view of algorithms that perform online reconstruction.}
	\label{fig_algorithms_brunel}
\end{figure}

The real-time processing software consists of algorithms which do a specific piece of the reconstruction of the particle paths. In addition to the reconstruction, there are other algorithms which do the selection of interesting events. Since the selection algorithms are generally a lot faster than reconstruction algorithms, they are excluded them from this discussion.

The general principle behind reconstruction is that a list of hits (where a pixel or silicon strip was hit) are given, and regular alignment of the hits indicate the path of the particle: hence the name \textit{pattern recognition}. Once all particles have been found by gathering the hits they created, they can be identified and fed into the selection decision making.

The information on which pixels and strips were hit comes straight from the detectors, in a heavily compressed binary format. It has to be first decoded to give the position of the individual strips, a process done by the \textbf{VELO decoder}, \textbf{UT decoder} and \textbf{FT decoder}.

Once pixel hit positions are known by their global X,Y,Z coordinates, the \textbf{VELO tracker} find hits that align to form a straight line. It estimates the starting position and the slope of the line. The line is extended into the UT, where the \textbf{VELO-UT tracker} searches silicon strip hits that are very close to the extended line. From the slight offset of the strip hits from the extended line, the VELO-UT tracker can estimate the amount of bending in the magnetic space, thus the momentum of the particle. The last step of tracking is done by the \textbf{forward tracker}, which uses the momentum estimate to extrapolate the bent path of the particle through the magnet, to the layers of the FT. It then finds silicon strips of the FT which align into a straight line (no magnetic field in the FT either), the line having the right direction and position to be a possible path the particle took after going through the magnet.

Finally, the \textbf{fitter} uses all the hits to align the particles path more closely with the position of the hits. In principle, it works similarly to a curve fitting solution, but uses a Kalman filter internally. Having the most accurate path, the decision is made to keep or drop the event.

\subsection{The aim of this thesis project}

As mentioned, the abandoning of the hardware trigger stage highly increases the load on the software trigger. The main goal of this project is to optimize the current software trigger to make it about 3 times as fast. Failure to do so will result in valuable events being dropped, thus reducing the physics potential of the experiment.

Current computing hardware has changed significantly from the ones the software trigger was originally made for. The even larger gap between memory and CPU speeds demands a more efficient use of CPU caches. Additionally, CPU instruction sets now include SIMD operations, which can, for example, do 4 floating point operations in place of one in the same amount of time. Furthermore, modern CPUs have a complex logic for branch prediction and instruction pipelining, which require code to be tailored to serve them.

To exploit the full capability of current hardware, not only individual pieces of the trigger software need to be changed, but the global data flow also has to be rethought and optimized.

During this thesis project, I will be helping the LHCb collaboration to reach its optimization goals for the software trigger.

%----------------------------------------------------------------------------
% === TBD ===
%----------------------------------------------------------------------------
\newpage
\section{Choosing optimization targets}

\color{red}
Explain the choice of initial choice of algorithms, based on the pie chart diagram and logical reasoning of our goals (i.e. what's needed).
\color{black}

As mentioned in \ref{sec_reco_sw_overview}, the reconstruction consists of individual algorithms which account for the bulk of the computation. (Scheduling the algorithms and culling decisions account for a much smaller CPU load.) It is straightforward to first start optimizing the algorithms which take the largest chunk of available computing power.

\begin{figure}[H]
	\begin{center}
		\includegraphics[width=0.6\textwidth]{algo_usage_original_bestphys}
	\end{center}
	\caption{Workload split among HLT1 algorithms.}
	\label{fig_algo_usage_choice}
\end{figure}

Looking at figure \ref{fig_algo_usage_choice}, we can see that the parametrized Kalman fitter takes nearly half the CPU budget, followed by the forward tracking which takes roughly a quarter. Based on this and initial performance profiling of the algorithms for hotspots, I decided to first examine and optimize the Kalman fitter. 

%----------------------------------------------------------------------------
% === TBD ===
%----------------------------------------------------------------------------
\section{Parametrized Kalman Fitter}

As described in \ref{sec_reco_sw_overview}, the track is reconstructed incrementally, start with velo hits, extended by UT hits and finally adding the FT hits. This process, however, is not so accurate. This manifests itself in the creation of \textit{ghost tracks} and missed tracks, and generally, tracks are only roughly aligned with the hits they were made from. Ghost tracks are tracks that did not exist in the real collision, they are merely artifacts of the reconstruction algorithms. As such, ghost tracks are highly undesirable, but this is where the Kalman fitter comes into play. 
The Kalman fitter basically refines the rough tracks that are spit out by preceding algorithms. The state of a particle can be described by its position, direction, and the quotient of its charge and momentum. The Kalman fitter first estimates the particle's state at its birth position based on the Velo hits alone. After that, it extrapolates the state of the particle to the next hit, or in other words, simulates the particle's travel until the next hit using the laws of physics. The new, \textit{predicted} state will have some deviation to the \textit{observed} state (that is, the hit), however, the Kalman fitter can make a mathematically optimal estimate for the true state based on the prediction and observation. The very new optimal state estimate will then be extrapolated to the next hit again, and this repeats for all the hits of the track.
As a result, the estimated state or path of the particle aligns more closely with the observed hits. In the case of ghost tracks, we can expect to have large deviations between the optimal estimated states and the observed hits, which could slipped through initial reconstruction algorithms but show up for the fitter. Such tracks are removed from the list of tracks, and that's why fitting is important.

\subsection{Performance profiling for hotspots in the Kalman fitter}


\begin{figure}[H]
	\begin{center}
		\includegraphics[width=0.9\textwidth]{kalmanfit_overall_breakdown}
	\end{center}
	\caption{Hotspots, or which parts of the Kalman fitter takes most of the time. Measured by Intel VTune Amplifier XE. \textcolor{red}{MAYBE add appendix explaining profiling and vtune}.}
	\label{fig_kalman_vtune_initial}
\end{figure}

Figure \ref{fig_kalman_vtune_initial} shows what fraction of the CPU time is spent in each individual function of the code. We can nicely see how the theoretical steps of the Kalman fitting map to the functions:
\begin{itemize}
	\item LoadHits: acquires position and measurement error of hits
	\item PredictState: extrapolates the state to the next hit
	\item UpdateState: makes an optimal estimate for the true state using the predicted state and the measured hit
	\item AverageState, ExtrapolateToVertex, etc.: various operations
\end{itemize}
There is a major and obvious problem however: just acquiring the data on which the computation is done should not take over 50\% of the Kalman fitting, but more like 1\%.	

\subsection{Loading hits in detail}

Careful examination reveals the way hits are loaded through the so-called \textit{Measurement providers}.
	
\begin{figure}[H]
	\begin{center}
		\includegraphics[width=0.6\textwidth]{kalmanfit_loadhits_schematic}
	\end{center}
	\caption{Illustration of how hit information is acquired from the array of LHCbIDs stored inside the Tracks. Contiguous array of clusters correspond to contiguous DRAM memory regions, while distinct objects, i.e. measurements have no spatial locality.}
	\label{fig_kalmanfit_loadhits_schematic}
\end{figure}

When a particle hits a detector, the identifier of the element of the detector that was hit is recorded. (Detector elements are analogous to the pixels of a digital CCD camera.) These elements are basically unambiguously identified by the so-called \textit{LHCbIDs}, so it is enough to store the IDs inside the Track object and all information (such as location of the hit, measurement error) can be recovered. 

Over the years however, this system grow unnecessarily complex resulting in a dramatic slowdown. Clusters, containing some basic information about the hit, such as its location, are stored inside measurement providers as a large array. In order to find the cluster that corresponds to the ID, this whole array is searched linearly. Once the cluster is found, a \textit{Measurement} object is allocated on the heap and initialized from it. Finally, another object, called a \textit{Trajectory}, is queried from the measurement, from which the data actually required can be extracted. The storage of clusters and creation of measurements is handled by \textit{MeasurementProviders}. Additionally, we can distinguish separate measurement objects for the Velo, UT and FT hits.

As seen, this is a convoluted process, involving an asymptotically unacceptable linear search and a lot of dynamic memory allocation. Dynamic allocation is not only slow, it highly suffers from thread contention at the operating system level in our multi-threaded software. Additionally, the individually allocated objects are scattered around in memory, resulting in poor CPU cache performance \textcolor{red}{MAYBE add appendix explaining caches}.

\begin{figure}[H]
	\begin{center}
		\includegraphics[width=0.9\textwidth]{kalmanfit_loadhits_breakdown}
	\end{center}
	\caption{Breakdown of CPU usage of the LoadHits function}
	\label{fig_kalmanfit_loadhits_breakdown}
\end{figure}

Figure \ref{fig_kalmanfit_loadhits_breakdown} clearly shows an excerpt from the CPU profiler and helps to understand where LoadHits spends its time. The most obvious thing is the std::find\_ifs that take nearly 60\% of the entire time of LoadHits. This corresponds to the linear search among clusters. The rest of the overhead comes from various boilerplate code, clear trends cannot be understood, but the volume of the overhead is seen to be significant.

\subsection{Simplifying the data loading}

To avoid this long chain to acquire the required data, the hits should be directly stored inside the Track rather than only by their IDs. Ideally, this would not incur any performance penalty, since the algorithms preceding the Kalman fitter all use the position and error information associated with a hit, so the detector element identifier is fully decoded anyway.

As described, the Track object has the following content (largely simplified):
\begin{lstlisting}
	struct Track {
		std::vector<LHCbID> ids;
	};
\end{lstlisting}

In the new model, the following structure is used:
\begin{lstlisting}
	struct TrackHit {
		Vector3D beginPosition;
		Vector3D endPosition;
		float errorX;
		float errorY;
	};
	
	struct Track {
		std::vector<LHCbID> ids;
		std::vector<TrackHit> veloHits;
		std::vector<TrackHit> utHits;
		std::vector<TrackHit> ftHits;
	};
\end{lstlisting}

Notice how the IDs are kept: the unfortunate reason for this is that other algorithms rely on these, and they cannot be removed in this first iteration. This structure, however, completely eliminates clusters, measurement and trajectories from the chain, and the Kalman fitter reads the contiguously stored information straight out of the track. This does not stress the memory allocator and is friendly for the caches.


\subsection{Performance profiling of the simplified model}


\begin{figure}[H]
	\begin{center}
		\includegraphics[width=0.9\textwidth]{kalmanfit_simplified_overall_breakdown}
	\end{center}
	\caption{Breakdown of the Kalman fitter after the simplified data loading}
	\label{fig_kalmanfit_simplified_overall_breakdown}
\end{figure}

Figure \ref{fig_kalmanfit_simplified_overall_breakdown} shows that with the new data model, the previous CPU hog, LoadHits, has completely disappeared, now accounting only for 2\% of the fitting.

As the parametrized Kalman fitter takes about 47\% of the entire reconstruction sequence, and about 50\% of the fitter's computing load was removed by the above described code changes, we would expect an overall speedup of 31\%. When measured, throughput increases from 4450 events processed per second to about 4850 events/second, or about 9.2\%. As this is way less than the predicted 31\%, the question arises as to where the performance is gone. First of all, three new data members were added to the Track to store the new TrackHits, and nothing has been removed. As Tracks are copied in the code, the three std::vectors also have to be copied, which involves dynamic memory allocation (a well-known performance drag) and memory copying. Second, part of the code that produces the TrackHits from other objects was not removed, but merely moved out of the fitter to other algorithms. The data conversion to TrackHits, along with adding the TrackHits to the vectors and allocating the memory of the vectors adds additional overhead. In order to achieve the projected performance improvements, these issues have to be fixed and optimized.


\subsection{High level optimizations}

Besides the TrackHits (or IDs), the Track contained three additional dynamically allocated std::vectors, which were filled with valid data but were not necessary from a computing point of view. Removing these data members confirmed the hypothesis by which the additional data members in the track slowed down the algorithm sequence: I observed an increase of 17\% in throughput (on top of the 9.2\%) when removing these members. While this can be regarded as an optimization independent to the fitter itself, it gains back the speed lost with the additional members required by the fitter.

\subsection{Micro-optimizations}

To trade physics quality for performance, event culling decisions can be made earlier in the reconstruction sequence, before fitting. This results in fitting taking a lot smaller part of the entire sequence while other algorithms become more prevalent. My work, although sped the fitting up, slightly slowed other algorithms down. Consequently, the \textit{best physics} case experienced a large increase in throughput, but the \textit{best throughput} case got slightly slowed down. In an attempt to restore the performance of the other algorithms, I had to further analyze performance.


\begin{figure}[H]
	\begin{center}
		\includegraphics[width=0.6\textwidth]{algo_usage_original_bestthru}
	\end{center}
	\caption{Distribution of CPU time among algorithms in the \textit{best throughput} case with early event culling}
	\label{fig_algo_usage_original_bestthru}
\end{figure}


Figure \ref{fig_algo_usage_original_bestthru} shows the in the best throughput case, the pixel tracking algorithm takes the most amount of time. This algorithm is responsible for finding particle track stubs from only Velo hits, and was negatively affected by my fitter optimizations.


\begin{figure}[H]
	\begin{center}
		\includegraphics[width=0.9\textwidth]{kalmanfit_disasm_opt_src_naive}
	\end{center}
	\begin{center}
		\includegraphics[width=0.9\textwidth]{kalmanfit_disasm_opt_asm_naive}
	\end{center}
	\caption{Code snippet from the profiler which shows the code I added to the pixel tracking algorithms in blue highlight. The upper image shows the C++ source code, the lower image shows the corresponding x86-64 disassembly.}
	\label{fig_kalman_disasm_src_naive}
\end{figure}

Looking at the disassembly, we can see two \textit{CALL} instructions, which correspond to the two function calls \textit{MakeFitterHit} and \textit{emplace\_back}. This means that the functions haven't been inlined. Inlining[\textcolor{green}{ref}] is a complex topic, because it can make code faster by removing function prologues[\textcolor{green}{ref}], but excessive inlining can also make code slower by polluting the instruction caches with two many repeated code snippets. In this case, the latter is unlikely, since these function are only present at this location, so inlining would be preferable.

\begin{figure}[H]
	\begin{center}
		\includegraphics[width=0.9\textwidth]{kalmanfit_disasm_opt_src_inlined}
	\end{center}
	\begin{center}
		\includegraphics[width=0.9\textwidth]{kalmanfit_disasm_opt_asm_inlined}
	\end{center}
	\caption{Source code and disassembly after manually inlining \textit{MakeFitterHit} inside the \textit{for} loop.}
	\label{fig_kalman_disasm_src_inlined}
\end{figure}

As can be seen on figure \ref{fig_kalman_disasm_src_inlined}, both \textit{CALL} instruction have disappeared. The first one for \textit{MakeFitterHit} due to the manual inlining, and the second for \text{emplace\_back} because of the compiler automatically inlining it. Note that the automatic inlining was enabled by passing the constructor arguments of \textit{TrackHit} to \textit{emplace\_back} instead of the ready object, exactly as \textit{emplace\_back} was meant to be used. Now, theoretically, the compiler could optimize out both cases as their semantics are equivalent, but it is apparently not capable of doing so.

Besides the absence of function calls, there is another thing noticeable on the assembly instruction. There is a large number of instructions moving quad words (\textit{MOVSD}), that is 64 bit double precision numbers. Furthermore, the \textit{CVTSS2SD} instructions are converting 32 bit single precision numbers to 64 bit doubles. Looking at the source code, we can indeed notice that input data from which the \textit{TrackHit} is made is stored as single precision floats, but the \textit{TrackHits} themselves are double precision because the fitter is using double precision calculations. Changing \textit{TrackHit} to store single floats as well, thus delaying the conversion, will hurt performance at another place where it has less of an impact.


\begin{figure}[H]
	\begin{center}
		\includegraphics[width=0.9\textwidth]{kalmanfit_disasm_opt_asm_nocvt}
	\end{center}
	\caption{Disassembly after changing \textit{TrackHits} to store single floats.}
	\label{fig_kalman_disasm_src_nocvt}
\end{figure}


The disassembly on figure \ref{fig_kalman_disasm_src_nocvt} clearly shows that the single precision to double precision conversions are gone just like the \textit{PXOR} instructions, and now it only moves double word memory units. With this little change, I managed to throw out lots of unnecessary instruction and the amount of memory moved around is also smaller.

Due to the complex interactions inside modern, pipelined CPUs and between the CPU, the DRAM and caches, it is hard to explain how and why the changes affected the performance. Nevertheless, inlining and trimming the assembly code has increased performance of the \textit{best throughput} scenario with early event culling from 13300 events per second to around 13700. Notably, the basic case without my code changes has produced about 14500 events per second. \small (As measured with my development branch on our performance test machines during development.) \normalsize



\subsection{Results, conclusion}

I managed to significantly increase the throughput of the \textit{best physics} case from 4450 events per second to 5870 events per second, or a 32\% increase. Unfortunately, the \textit{best throughput} case slowed down from 14500 events/sec to about 13700, or 5\% decrease in throughput.

\begin{figure}[H]
	\begin{center}
		\includegraphics[width=0.7\textwidth]{kalmanfit_throughput_results_lowquality}
	\end{center}
	\caption{Throughput of the particle path reconstruction before and after my modifications, for the \textit{best physics} and \textit{best throughput} cases.}
	\label{fig_kalmanfit_results_throughput}
\end{figure}

Another interesting figure to look at is how the weight of the parameterized Kalman fitter in the entire algorithm sequence has changed. Previously, it took 47\% of the whole sequence, and now it only takes 22\%. If in addition the fact that the whole sequence is significantly faster is factored in, the fitter is well above two times faster. \small (These type of measurements are to be taken with a grain of salt because of complex system interactions and the consequent inaccuracy of profiling, but are interesting and provide a good general view.) \normalsize

\begin{figure}[H]
	\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=.9\linewidth]{algo_usage_original_bestphys}
		\caption{Before optimizations}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=.9\linewidth]{algo_usage_optkalman_bestphys}
		\caption{After optimizations}
	\end{subfigure}
	\caption{The distribution of processor time requirements of each algorithms. The parameterized Kalman fitter is identified by the label \textit{ForwardFitterAlgParamFast}.}
	\label{fig_kalmanfit_results_algousage}
\end{figure}

In light of the code changes and their effect on the overall performance, we can safely say that code should strive to do the data transformations in the simplest possible way. Adding extra layers on top, if not done carefully using zero-cost abstractions, will dramatically slow the code down. In performance critical applications, a good data oriented design can give far better benefits that assembly-level micro-optimizations. Additionally, a good data structures opens up the doors to more effective micro-optimizations, such as vectorization[\textcolor{green}{ref}].





%----------------------------------------------------------------------------
% === TBD ===
%----------------------------------------------------------------------------
\section{Streamlining the computation's data model}



\section{Vectorizing and optimizing the Velo-UT algorithm}\label{sec_opt_velout}


As highlighted in \ref{sec_reco_sw_overview}, the Velo-UT algorithm extends the straight line tracks created in the VELO tracking algorithm by assigning UT silicon strip hits to it. More importantly, the Velo-UT algorithm provides a course estimate for the momentum of the particle, allowing efficient tracking in the FT.
The algorithm takes roughly 10 percent of the pipeline, so optimizing it is not expected to provide a high overall gain in performance, however its code is outdated and is in much need of an overhaul. My goal for the Velo-UT was to streamline the code to adhere to modern coding practices, and in the meantime, to make the algorithm play well with modern CPU architectures. In light of this, I was aiming at a highly SIMD-vectorized\textcolor{red}{TODO: add appendix} code, which operates with SOA\textcolor{red}{TODO: add appendix} data structures

\subsection{Geometry of the UT detector}

\begin{figure}[H]
	\begin{center}
		\includegraphics[width=0.7\textwidth]{detector_ut_geometry}
	\end{center}
	\caption{Illustration of the key elements of the UT hardware.}
	\label{fig_ut_geometry}
\end{figure}

Figure \ref{fig_ut_geometry} shows the rectangular panels of the UT as looking down the detector from the VELO. The UT detector consists of four panels, their placement in the entire detector can be seen on figure \ref{fig_lhcb_geometry}. Each of the four panels is made up 10 cm by 10 cm sensors. Though both the front two and rear two panels contain 14 rows of sensors, the rear panels are wider. The middle two panels have their sensors tilted by +5 and -5 degrees, which helps to mitigate the poor vertical resolution resulting from using silicon strips instead of pixel detectors. Each sensor contains silicon strips that run the full length of 10 cm of the sensor. There are 512 strips next to each other on every sensor. The exceptions are the sensor near the center of the panels, since the ones marked in yellow contain 1024 strips, and the ones marked in red contain 1024 half-length strips. The higher density area in the middle is required to deal with increased luminousity in the axis of the beam pipe. For each collision event, the software receives a list of the silicon strips that were hit by a particle. The Technical Design Report for the LHCb tracker\cite{tracker_tdr} explains the geometry in more detail.

\subsection{Decoding of the raw data from the detector}

Each silicon strip has a unique numeric identifier. During a collision event, the electronics of the UT detector compile a list of identifiers of the strips that were hit by a particle, which the software receives in a heaavily compressed format. With a description of the exact geometry of the detector, that is, the location and dimension of sensors and strips in the global 3D space, the software is able to represent the strips that were hit by a line segment in the 3D space instead of just an ID. The UTHits resulting from the  decoding process are gathered into a HitHandler structure, which groups the hits by which sensor they belong to. The Velo-UT algorithm consumes the HitHandler alongside the VELO tracks to produce \textit{upstream} tracks.


\subsection{The original Velo-UT algorithm}\label{sec_velout_desc}

The main idea of the Velo-UT algorithm is to consume the VELO tracks and the UT hits, and extend the VELO tracks with hits from the UT to create so called \textit{upstream} tracks. The algorithm handles each VELO track separately. The process for a single VELO tracks can be broken down into the following steps:

\begin{enumerate}
	\item Skip track if unstuitable
	\item Extrapolate VELO track to the UT detector
	\item Gather UT hits that are close to the extrapolated track
	\item Find 3 or 4 UT hits that align in a line
	\item Analyze the \textit{track candidates} that consist of the VELO track + 3-4 UT hits
	\item Select the best candidate (if any) to extend the VELO track
\end{enumerate}

\subsubsection{Filtering tracks}

Any kind of tracks can be fed into the algorithm from the VELO tracking. Some tracks point to the wrong direction, away from the UT, other tracks go very close to the beamline and hit the central hole in the UT's panels, and some tracks may also simply miss the UT's panels as they go too much to the sides. These tracks are not worth trying to extend because there will be no solution, so they are dropped.

\subsubsection{Extrapolating tracks}

Tracks from the VELO are described by the \textit{state} of the particle. The state is a tuple of x,y and z coordinates, and the slopes tx=dx/dz and ty=dy/dz. In other words, the particle has a position [x,y,z] and has a velocity vector in the direction [tx,ty,1]. The usual coordinate system (see \ref{sec_coordinate_system}) is used for the states as well. To gather UT hits, one has to know where a track crosses a particular panel of the UT, which can be calculated by extrapolating the x,y,z position of the state to the z position of the panel along to line given by the slopes.

\subsubsection{Gathering UT hits}

Once the state is extrapolated to the z coordinate of a panel, one can collect hits that are close to the x and y position of the extrapolated state. The implementation of the algorithm first collects the list of the 10 by 10 centimeter sensors that fall close enough to the extrapolated state. In case the track passes through the middle of one sensor, only one sensor will be tagged, however if the track hits the corner between four sensors, all the four may be tagged. Afterwards, the algorithm iterates through all the hits that belong to the tagged sensors, and checks for each hit if they are within tolerance to the extrapolated state. This, including the extrapolation, is repeated for all four panels, thus the final result is a set of close-by hits for each of the four panels.

\subsubsection{Finding aligned hits}

There are on average 2-3 hits returned for each panel. Among these hits, the algorithm tries to find 3 or 4 hits that form a fairly straight line. These are called track candidates, and since there may be multiple sets of hits that form a straight line, the algorithm must decide on which is the best candidate.

\subsubsection{Analyzing candidates}

Each candidate that belongs to a track goes through a fitting stage. This is much like curve fitting or linear regression, the algorithm find a mathematically optimal set of parameters that best describe the path of the particle that have created the UT hits of the candidate. 

\subsubsection{Selecting the best candidate}

The error of the fitting of a candidate can be calculated by determining how far the parameterized path of the particle lies from the position of the UT hits it was fitted from. If any of the candidates have a sufficiently low error, the one with the lowest is picked to form the newly created upstream track.


\subsection{Analysis of the original Velo-UT algorithm}

\subsubsection{Hotspots}

\begin{figure}[H]
	\begin{center}
		\includegraphics[width=0.7\textwidth]{velout_hotspots_orig}
	\end{center}
	\caption{Resolts of hotspots profiling with Intel VTune.}
	\label{fig_velout_hotspots_orig}
\end{figure}

As we can see, the Velo-UT algorithm spends most of its time gathering the UT hits that could potentially belong to a VELO tracks. The second largest portion is \textit{formClusters}, which performs the search for aligned UT hits as well as a preliminary fit to determine the best candidate. The third largest unit is the creation of the output (the \textit{upstream} tracks), which performs a higher quality final fit and does some analysis to remove potentially undesirable tracks.

In light of this, the primary target for improvement is the largest chunk: gathering the hits, however, the other hotspots are also of significant interest.


\subsubsection{Gathering hits}

Examining the code of the hit gathering function(\ref{lst_velout_gatherhits_original}), we can see that the main idea is to loop over the four planes, extrapolate the track to the plane, and collect the hits for each plane.

The first step in locating the close-by UT hits happens by finding the sectors (that is, the 10-by-10 centimeter sensors) that are adjacent to the point where the extrapolated track hits the current panel. The \code{findSectors} function performs this step via a rather complex logic (which is not shown here).

Given the sectors of interest, a loop goes over the sectors and queries the list of hits that belong to that sector from the \code{HitHandler}. The \code{HitHandler} already has the UT hits grouped by sectors, so this operation is rather simple.

Finally, the \code{findHits} function loops over the hits that belong to one sector, and keeps the hits that are truly close to the extrapolated track. Truly close here means that the fiber, when fattened into a cylinder, intersects with the line of the extrapolated track. (In reality, a simpler x and y tolerance is used instead of the heavy ray-cylinder intersection calculations.)

The final result is an array which contains four arrays of hits, one for each panel.

\vspace{1pc}

Looking at the code, we can spot several issues:
\begin{enumerate}
	\item Getting the list of nearby sectors is complicated: the sectors do not form a regular grid because of the smaller ones in the middle, which needs to be handled when doing lookups by x,y coordinates.
	\item Found sectors may be duplicated, as indicated by line 319-320, which is handled by special cases again.
	\item Early exit conditions to cut computation time on lines 299, 302 and 320.
\end{enumerate}

Issues \#1 and \#2 indicate that more work is done than absolutely necessary, while branching caused \#2 and \#3 may directly affect the performance on modern superscalar CPUs with deep pipelining. The idea with early exit conditions is to skip computation that will prove useless, however if the CPU misses the branch prediction, it has to flush its entire pipeline and restart the other branch. It is often a better approach to vectorize the branchless code and do the unnecessary calculations as well, then filter the only the final results (with vectorized code). Overall, the code is not vectorized at all, and does not have data-structures that would allow for vectorization.

In addition to performance problems, as the code is not clearly structured, optimization opportunities are easy to miss and hard to exploit, for humans and compilers alike.

\begin{figure}[H]
	\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=.7\linewidth]{velout_gatherhits_orig_uarch}
		\caption{Microarchitecture usage of gathering\\ hits in the original code}
		\label{fig:sub1}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=.7\linewidth]{velout_gatherhits_new_uarch}
		\caption{Microarchitecture usage of gathering\\ hits in the refurbished code}
		\label{fig:sub2}
	\end{subfigure}
	\caption{Almost 30\% of the instructions go to waste due to CPU branch mispredictions before refurbishing the code. (Illustration, in-depth analysis later.)}
	\label{fig_velout_branch_example}
\end{figure}


\subsubsection{Finding aligned hits}

The largest part of the hit finding function is in fact the fitting of the alignging track candidates. Fitting is a simple, linear stream of mathematical operation with no branching, thus makes a perfect candidate for vectorization. However, vectorization cannot be exploited unless it is completely separated from the alignment search.

\begin{figure}[H]
	\begin{center}
		\includegraphics[width=0.7\textwidth]{velout_hotspots_orig_formclusters}
	\end{center}
	\caption{The breakdown of the aligned hit searching. The two simpleFit calls account for the fitting, while the remaining ~30\% is the alignment search.}
	\label{fig_velout_hotspots_orig_formclusters}
\end{figure}


\subsubsection{Creating the final track}

The data format of the final track is not optimal. A track is represented by its own class, which dynamically allocates memory for the small number of hits and other objects it has. Transforming the track into a structure of arrays format is something to consider, which means for example that all the hits for all the track would reside in a large array, and the individual tracks would be indices into that array.
Additionally, as part of the creation of the track, a final, more accurate fit is performed. This is also a prime candidate for vectorization when separated from the rest of the code, and may also be merged with the preliminary fit to provide simpler and faster code, and potentially better results.

\subsubsection{General code design}

The code violates several coding best practices, which is not strictly an optimization problem, but is still an important concern as it is hard to find optimization opportunities in code that is hard to reason about. The most severe breaches are the single responsibility and dependency inversion priciples. An example for the former is \href{https://gitlab.cern.ch/lhcb/Rec/blob/1b7edc5aea96f2225601238b3f64e478e41b6c70/Pr/PrVeloUT/src/PrVeloUT.cpp#L337}{formClusters}, which not only find aligned hits but also parametrically fits them, an example for the latter is \href{https://gitlab.cern.ch/lhcb/Rec/blob/1b7edc5aea96f2225601238b3f64e478e41b6c70/Pr/PrVeloUT/src/PrVeloUT.h#L191}{simpleFit}, which modifies the flow control of its caller (formClusters) via modification of in-out (quasi-global) parameters. When the problems are alleviated, the search for aligned hits and the fitting can be done completely separately, which provides a massive gain thanks to the perfect vectorization of the fitting process.


\subsection{Design of the improved algorithm}

\subsubsection{Uniform grid space partitioning}

For the logic part, I left the algorithm as it was as I could not find better alternatives \textcolor{red}{(maybe explain parabola and hough methods I tried)}, however I redesigned space partitioning from scratch.

Space partitioning, most prominent in 3D graphics and physics simulations, is a technique to accelerate computations that are done on a large number of objects contained in a 3D space. Frequently, there are only significant intereactions between nearby objects, so computations for the interactions of far away objects can be neglected. To be able to quickly list objects nearby to a 3D coordinate, a space partitioning scheme is used. Arguably, the simplest sceheme is uniform grid space partitioning, which slices the 2D or 3D space by a uniform grid, and keeps a list of objects for each grid cell that fall into the cell. This way, to find objects near to a specific point, one only has to calculate the grid cell the point falls in (a few divisions) and read the list of that cell. (In some cases, the nearest four cells can be used not to miss any objects in case the specified point lies close to a grid line.)

To reduce the complexity of finding the nearby sectors of a plane for an extrapolated track, I replaced the sector-based lookup by a uniform grid-based lookup. Since there are 14  rows of sensors on every plane of the UT detector, I used a grid with 14 rows, carefully aligned to the geometrical position of the sensor rows. This ensures that a fiber falls entirely within its grid cell. For the half-height sensors in the middle, two sensors fall into the same grid cell. There are no such limits for the horizontal resolution of the grid cells, so the cell width can be set arbitrarily. As the search region in the order of a few centimeters, I chose to have 64 cells on the horizontal axis, resulting in a cell width of roughly 3 cm. The third axis in the direction of the beamline is divided so that each four panel of the UT falls into its own grid cell. To sum up, the grid is 64 x 14 x 4 cells.

\textcolor{red}{add a figure for space partitioning}

The binning (assignment of cell) of hits is done by taking the middle point of the fiber, and using a division on all three axes to determine the integer coordinates of the corresponding cell.
Similarly, lookup of hits nearby to a point involves first a similar division to acquire the corresponding cell, then the hits assigned to that cell can be iterated over. One caveat, however, is that the middle two layers are tilted, so some tilted fibers cross multiple cells. In this case, we would like to have them show up when we query either of the cells. Instead of duplicating hits, I chose to simply sample the nearbest two cells instead of just one. With the cell size of 3 centimeters, it is unlikely to miss a hit in such way. The same issues persists in the vertical direction as well when the sampled point falls very close to a grid line. This is exacarbated by the small displacement on the Z axis that the sensors have by design of the hardware, and manufacturing errors also add to the problem. However, by design, there is a small overlap between two adjacent sensors, both on the vertical and horizontal axes. This means that a track hitting a grid line is likely to excite two fibers at the same time. The idea is that even when avoiding double-sampling on the vertical axis, at least one of those fibers will still be registeredd in the subject cells.

\subsubsection{Data structures and steps}

\begin{figure}[H]
	\begin{center}
		\includegraphics[width=0.85\textwidth]{velout_opt_datastruct_flow_p1}
	\end{center}
	\caption{Data structures and process diagram of the new Velo-UT algorithm. (Part 1)}
	\label{fig_velout_opt_datastruct_flow_p1}
\end{figure}

\begin{figure}[H]
	\begin{center}
		\includegraphics[width=0.7\textwidth]{velout_opt_datastruct_flow_p2}
	\end{center}
	\caption{Data structures and process diagram of the new Velo-UT algorithm. (Part 2)}
	\label{fig_velout_opt_datastruct_flow_p2}
\end{figure}

\paragraph{Layout of data}

Figures \ref{fig_velout_opt_datastruct_flow_p1} and \ref{fig_velout_opt_datastruct_flow_p2} 

\subsubsection{Vectorization of extrapolation and fitting}

\subsubsection{Vectorization of gathering hits}

\subsubsection{No vectorization for hit alignment search}

\subsubsection{Custom memory allocation}


\subsection{Results}

\subsubsection{Overall performance}

\subsubsection{Microarchitecture usage, bottlenecks}

\subsubsection{Effects of customized memory allocation}

\subsubsection{Comparison of Clang and GCC}



%----------------------------------------------------------------------------
% === Conclusion ===
%----------------------------------------------------------------------------
\section{Conclusion}

\color{red}
- summarize my own contributions \\
- summarize achieved results \\
- make conclusions about them \\
- how it affects the future \\
BRIEFLY
\color{black}


%----------------------------------------------------------------------------
% REFERENCES
%----------------------------------------------------------------------------
\section{References}

\begin{thebibliography}{asd}
	\bibitem{cern_about} About CERN: \\
		\url{https://home.cern/about}
	\bibitem{cern_accel_complex} The accelerator complex: \\
		\url{https://home.cern/about/accelerators}
	\bibitem{lhc_desc} About the Large Hadron Collider: \\
		\url{https://home.cern/topics/large-hadron-collider}
	\bibitem{lchb_desc} About the Large Hadron Collider beauty experiment: \\
		\url{https://home.cern/about/experiments/lhcb}
	\bibitem{lhc_lead_ions} Why collide lead ions: \\
		\url{http://alicematters.web.cern.ch/?q=FAQ-why-lead-ions}
	\bibitem{lhc_energy} Energy of the LHC: \\
		\url{https://home.cern/about/engineering/restarting-lhc-why-13-tev}
	\bibitem{lhc_bunch_collisions} LHC collisions: \\
		\url{https://lhc-machine-outreach.web.cern.ch/lhc-machine-outreach/collisions.htm}
	\bibitem{lhc_facts_and_figures} LHC facts and figures: \\
		\url{https://public-archive.web.cern.ch/en/LHC/Facts-en.html}
	\bibitem{tracker_tdr} Tracker (UT and FT) Technical Design Report: \\
		\url{https://cds.cern.ch/record/1647400?ln=en}
\end{thebibliography}


%----------------------------------------------------------------------------
% SCRATCH
%----------------------------------------------------------------------------
\newpage
\section{Scratch}

\lstinputlisting[
	caption=Hit gathering for the original Velo-UT,
	label=lst_velout_gatherhits_original,
	language=C++,
	firstline=277,
	lastline=332,
	firstnumber=277]
	{source_code/PrVeloUT.cpp}

\end{document}
