\documentclass[12pt]{article}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\graphicspath{ {./images/} }

\usepackage{listingsutf8}
\usepackage[utf8]{inputenc}

\usepackage[a4paper, total={16cm, 23.7cm}]{geometry}
\DeclareMathSizes{12}{13}{10}{8}
\setlength\parindent{0pt}

\usepackage[colorlinks=true,urlcolor=blue]{hyperref}
\usepackage{color}

\newcommand*\diff{\mathop{}\!\mathrm{d}}
\newcommand*\Diff[1]{\mathop{}\!\mathrm{d^#1}}
\newcommand\tab[1][.7cm]{\hspace*{#1}}
\renewcommand{\refname}{}

\fontfamily{phv}

% !TeX spellcheck = en_US


\begin{document}	
	
	%----------------------------------------------------------------------------
	% TITLE
	%----------------------------------------------------------------------------
	\begin{center}
		\Huge Optimization of the trigger software of the LHCb experiment\\		
		\Large Thesis requirements specification\\
		\vspace{1pc}
		\huge PÃ©ter Kardos \\
		\large 2018-2019
	\end{center}
	
	
	%----------------------------------------------------------------------------
	% BACKGROUND
	%----------------------------------------------------------------------------
	\section{Background}
	
	
	CERN\cite{cern_about} is a nuclear research organization that hosts the world's largest particle accelerator\cite{lhc_desc}. The accelerator is a circular tube, with a circumference of about 27km. Batches of protons are circulating in both directions, with the opposite directions intersecting at a few points, giving possibility for the proton beams to collide. This thesis project is focused on the software that processes the data collected during the collisions for the LHCb\cite{lchb_desc} experiment.
	
	\vspace{0.7pc}
	The amount of data generated for collisions is huge, however, only a small fraction of events are worth storing for further investigation. The purpose of the so-called \textit{trigger software} is to process the data in real-time to filter for interesting events, which are further processed offline.
	
	\vspace{0.7pc}
	Fast dedicated hardware electronics collect the data from the detector and build so called raw events. These are then decoded and analyzed in multiple stages on a large farm of standard servers. Particle trajectories are reconstructed and analyzed in order to decide whether the event is worth recording based on characteristics such as momentum and type of tracks. This process will happen at a rate of 30MHz from 2021 on, leaving only around 30us of node time for each event reconstruction on average. The size of the server farm will be in the order of 1000 nodes, each node having 40-100 CPU cores, which puts the scale of the challenge into perspective.
		
	\vspace{0.7pc}	
	As the main code is dating from early 2000s, the underlying framework itself (named Gaudi) was not prepared for such workload and environment. A lot of effort has been put into modernizing it in the past years but the performance is not satisfactory yet. The goal is to achieve a 3 times speedup of the code compared to its current state. (This means a 6 times speedup compared to the code before modernization.)
	
	\vspace{0.7pc}
	In order to achieve this ambitious goal, the current software of the LHCb experiment needs to be revamped to take full advantage of modern processor features. Many core systems, low level parallelization, SIMD vectorization and efficiently utilizing superscalar architectures are of particular interest.	
	
	\vspace{0.7pc}
	Failure to deliver required performance improvements would mean that the number of interesting events reaching the offline analysis stage would be reduced, limiting the physics potential of the overall experiment.	
	
	
	%----------------------------------------------------------------------------
	% DESCRIPTION
	%----------------------------------------------------------------------------
	\newpage
	\section{Description of the task}
	
	The thesis project is focused on optimizing the code running on an individual node of the compute cluster to fully utilize resources available on the node. Nodes host 40-100 modern AMD/Intel CPUs, run a recent Linux distribution, and operate independently of each other.
		
	\vspace{1pc}
	The work consists of the following main points:
	
	\begin{itemize}
		\item Benchmark existing code to measure efficiency of the current code base.
		\item Discover limitations using tools such as Cachegrind, perf and Intel VTune Amplifier.
		\item Make proposals to improve efficiency: a proposal can be rewriting subparts, reorganizing data structures, new processing algorithms, among others.
		\item Implement some of these optimizations.
		\item Measure improvements achieved.
		\item Share the acquired knowledge with colleagues so that they can make use of the same optimization techniques.
	\end{itemize}	
		

	%----------------------------------------------------------------------------
	% METHODS
	%----------------------------------------------------------------------------
	%\newpage
	\section{Methods}
	
	Programming environment:
	\begin{itemize}
		\item C++, latest specification
		\item Python
		\item Multi-core systems
	\end{itemize}

	Methods:
	\begin{itemize}
		\item AVX2 and/or AVX512 vectorization
		\item Multi-threading
		\item Cache and data layout optimization
		\item Vectorization using external libraries like VC or VCL
	\end{itemize}

	Evaluation:
	\begin{itemize}
		\item Cachegrind
		\item Intel Parallel Studio XE
		\item perf
		\item Manual timing measurements
	\end{itemize}



	%----------------------------------------------------------------------------
	% RELEVANT COURSES
	%----------------------------------------------------------------------------
	\newpage
	\section{Relevant courses}
	
	\begin{itemize}
		\item \href{http://www.uu.se/en/admissions/master/selma/kursplan/?kpid=31897&lasar=18%2F19&typ=1}
			{High Performance Programming}	
		
		\item \href{http://www.uu.se/en/admissions/master/selma/kursplan/?kpid=31898&type=1}
			{Parallel and Distributed Computing}
	\end{itemize}


	% NOTE ON CERN INTERNAL COURSES
	% LHCb and CERN are providing internally several courses that may be beneficial, in particular around C++, performance optimization and vectorization. We will see how you can benefit from them. One goal of this work can also be to deliver one of these courses at the end.

	
	
	
	%----------------------------------------------------------------------------
	% DELIMITATIONS
	%----------------------------------------------------------------------------
	\section{Delimitations}

	The work is solely focused on delivering CPU computing optimizations inside a single node. 
	
	As such
	\begin{itemize}
		\item GPU processing,
		\item developing new physics algorithms,
		\item and distributing the workload across nodes
	\end{itemize}
	is out of scope of this project.


	%----------------------------------------------------------------------------
	% TIME PLAN
	%----------------------------------------------------------------------------
	\section{Time plan}
		
	\begin{itemize}
		\item Learning the environment and the tools (\textbf{2 months})
		\item Benchmarking and drawing conclusions on the pieces to improve (\textbf{2 months})
		\item Implementing an improved version of a given piece of code (\textbf{3 months})
		\item Validating results and fine tuning optimization (\textbf{1 month})
	\end{itemize}

	
	%----------------------------------------------------------------------------
	% REFERENCES
	%----------------------------------------------------------------------------
	\section{References}
	
	\begin{thebibliography}{asd}
		\bibitem{cern_about} About CERN: \\
			\url{https://home.cern/about}
		\bibitem{lhc_desc} About the Large Hadron Collider: \\
			\url{https://home.cern/topics/large-hadron-collider}
		\bibitem{lchb_desc} About the Large Hadron Collider beauty experiment: \\
			\url{https://home.cern/about/experiments/lhcb}
	\end{thebibliography}

\end{document}





















